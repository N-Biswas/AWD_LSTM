{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NB_FellowshipAI_Twitter_ULMFiT.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"bS7k2AczixG7","colab_type":"text"},"cell_type":"markdown","source":["# Applying a ULMFiT model to Twitter US Airlines Sentiment Analysis"]},{"metadata":{"id":"ch9PCI1UjFqj","colab_type":"text"},"cell_type":"markdown","source":["***Introduction***\n","\n","Twitter US Airlines Sentiment provides a csv file of Twitter data scraped from Feburary 2015, which contains tweets regarding six major US airlines. In this project, we apply a supervised ULMFiT model to classfiy positive, negative and neutral tweets. ULMFiT is a transfer learning model, which essentialy aims to use a model that has been trained to solve one problem as the basis to solve a similar problem. \n","In this instance,  we fine tune a pre-trained language model (trained on Wikipedia data), and apply it to classify tweets. \n","\n","---\n","\n"]},{"metadata":{"id":"SMa-GzDpiucA","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n","!pip install fastai\n","import fastai\n","from fastai import *\n","from fastai.text import * "],"execution_count":0,"outputs":[]},{"metadata":{"id":"xrzheubTjbnt","colab_type":"text"},"cell_type":"markdown","source":["***EDA ***\n","\n","After installling the required packages, we begin by reading the csv as a pandas DataFrame and inspecting the structure of the data. It can be observed that sentiment analysis labels are positive (16%), negative (63%) and neutral (21%). We can also see that that body of the tweets are within the 'text' column. We therefore  transform the DataFrame to contain only the 'airline_sentiment'  and 'text' columns, stored as 'label' and 'text' respectively. These two columns will be the labels and features for our model. "]},{"metadata":{"id":"Et4juSsiiucM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":465},"outputId":"9dc4b187-ec2a-4a63-e855-6fd59c580893","executionInfo":{"status":"ok","timestamp":1552254321783,"user_tz":0,"elapsed":528,"user":{"displayName":"Nishit Biswas","photoUrl":"","userId":"09281587797812942318"}}},"cell_type":"code","source":["df = pd.read_csv('Tweets.csv')\n","df.head()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>airline_sentiment</th>\n","      <th>airline_sentiment_confidence</th>\n","      <th>negativereason</th>\n","      <th>negativereason_confidence</th>\n","      <th>airline</th>\n","      <th>airline_sentiment_gold</th>\n","      <th>name</th>\n","      <th>negativereason_gold</th>\n","      <th>retweet_count</th>\n","      <th>text</th>\n","      <th>tweet_coord</th>\n","      <th>tweet_created</th>\n","      <th>tweet_location</th>\n","      <th>user_timezone</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>570306133677760513</td>\n","      <td>neutral</td>\n","      <td>1.0000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Virgin America</td>\n","      <td>NaN</td>\n","      <td>cairdin</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@VirginAmerica What @dhepburn said.</td>\n","      <td>NaN</td>\n","      <td>2015-02-24 11:35:52 -0800</td>\n","      <td>NaN</td>\n","      <td>Eastern Time (US &amp; Canada)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>570301130888122368</td>\n","      <td>positive</td>\n","      <td>0.3486</td>\n","      <td>NaN</td>\n","      <td>0.0000</td>\n","      <td>Virgin America</td>\n","      <td>NaN</td>\n","      <td>jnardino</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@VirginAmerica plus you've added commercials t...</td>\n","      <td>NaN</td>\n","      <td>2015-02-24 11:15:59 -0800</td>\n","      <td>NaN</td>\n","      <td>Pacific Time (US &amp; Canada)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>570301083672813571</td>\n","      <td>neutral</td>\n","      <td>0.6837</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Virgin America</td>\n","      <td>NaN</td>\n","      <td>yvonnalynn</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n","      <td>NaN</td>\n","      <td>2015-02-24 11:15:48 -0800</td>\n","      <td>Lets Play</td>\n","      <td>Central Time (US &amp; Canada)</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>570301031407624196</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>Bad Flight</td>\n","      <td>0.7033</td>\n","      <td>Virgin America</td>\n","      <td>NaN</td>\n","      <td>jnardino</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@VirginAmerica it's really aggressive to blast...</td>\n","      <td>NaN</td>\n","      <td>2015-02-24 11:15:36 -0800</td>\n","      <td>NaN</td>\n","      <td>Pacific Time (US &amp; Canada)</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>570300817074462722</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>Can't Tell</td>\n","      <td>1.0000</td>\n","      <td>Virgin America</td>\n","      <td>NaN</td>\n","      <td>jnardino</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@VirginAmerica and it's a really big bad thing...</td>\n","      <td>NaN</td>\n","      <td>2015-02-24 11:14:45 -0800</td>\n","      <td>NaN</td>\n","      <td>Pacific Time (US &amp; Canada)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n","0  570306133677760513           neutral                        1.0000   \n","1  570301130888122368          positive                        0.3486   \n","2  570301083672813571           neutral                        0.6837   \n","3  570301031407624196          negative                        1.0000   \n","4  570300817074462722          negative                        1.0000   \n","\n","  negativereason  negativereason_confidence         airline  \\\n","0            NaN                        NaN  Virgin America   \n","1            NaN                     0.0000  Virgin America   \n","2            NaN                        NaN  Virgin America   \n","3     Bad Flight                     0.7033  Virgin America   \n","4     Can't Tell                     1.0000  Virgin America   \n","\n","  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n","0                    NaN     cairdin                 NaN              0   \n","1                    NaN    jnardino                 NaN              0   \n","2                    NaN  yvonnalynn                 NaN              0   \n","3                    NaN    jnardino                 NaN              0   \n","4                    NaN    jnardino                 NaN              0   \n","\n","                                                text tweet_coord  \\\n","0                @VirginAmerica What @dhepburn said.         NaN   \n","1  @VirginAmerica plus you've added commercials t...         NaN   \n","2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n","3  @VirginAmerica it's really aggressive to blast...         NaN   \n","4  @VirginAmerica and it's a really big bad thing...         NaN   \n","\n","               tweet_created tweet_location               user_timezone  \n","0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n","1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n","2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n","3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n","4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"uXjTmgOyiucS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"1e164d00-1b01-4bcb-9d20-1b105c27b840","executionInfo":{"status":"ok","timestamp":1552254744424,"user_tz":0,"elapsed":580,"user":{"displayName":"Nishit Biswas","photoUrl":"","userId":"09281587797812942318"}}},"cell_type":"code","source":["df.airline_sentiment.value_counts()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["negative    9178\n","neutral     3099\n","positive    2363\n","Name: airline_sentiment, dtype: int64"]},"metadata":{"tags":[]},"execution_count":4}]},{"metadata":{"id":"84o1yDCOiucY","colab_type":"code","colab":{}},"cell_type":"code","source":["df = pd.DataFrame({'label':df.airline_sentiment , 'text':df.text})"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_sYCZlWvmAyr","colab_type":"text"},"cell_type":"markdown","source":["***Cleaning/preprocessing the data***\n","\n","Before proceeding to the construction of the model, we must first clean the tweets and remove irrevelant content. Upon inspection, it can be seen that many of the tweets include hastags, mentions and other content not correctly encoded. \n","\n","As a first step we apply regular expressions to remove hastags but keep the word after the '#', as adjacent text may contain useful information about the tweet. Similarly regex is used to remove any hyperlinks and mentions, as they do not add value to sentiment analysis.\n","\n","It also appears that HTML encoding has not been converted to text. To account for this, we use BeautifulSoup to decode HTML. Other common methods in NLP analysis include the removal of stopwords and tokenisation of tweets but this was neglected, as it yielded lower accuracies. We encapsultate these ideas in the *clean_tweets* function. "]},{"metadata":{"id":"TZhsDGCtiucc","colab_type":"code","colab":{}},"cell_type":"code","source":["import re \n","from bs4 import BeautifulSoup\n","#function to clean a tweet, removing #s, mentions , URLs, and decode HTML\n","def clean_tweets(tweet):\n","    #remove hashes\n","    tweet = re.sub(r'#', '', tweet)\n","    \n","    #remove mentions\n","    tweet= re.sub(r'@[A-Za-z0-9]+','',tweet)\n","    \n","    #remove URLS\n","    tweet = re.sub('https?://[A-Za-z0-9./]+','',tweet)\n","    \n","    #HTML decoding\n","    tweet = re.sub('https?://[A-Za-z0-9./]+','',tweet)\n","    altered = BeautifulSoup(tweet, 'lxml')\n","    tweet = altered.get_text()\n","    \n","    return tweet "],"execution_count":0,"outputs":[]},{"metadata":{"id":"PbrS0qWynCs8","colab_type":"text"},"cell_type":"markdown","source":["To see the impact of the clean, we can view the head before and after preprocessing. "]},{"metadata":{"id":"tAZ9HSCEmn27","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"fcfd0f62-fa2c-471a-e1f2-6600334224f9","executionInfo":{"status":"ok","timestamp":1552254939597,"user_tz":0,"elapsed":559,"user":{"displayName":"Nishit Biswas","photoUrl":"","userId":"09281587797812942318"}}},"cell_type":"code","source":["#Data before preprocessing\n","df.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>neutral</td>\n","      <td>@VirginAmerica What @dhepburn said.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>positive</td>\n","      <td>@VirginAmerica plus you've added commercials t...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>neutral</td>\n","      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>negative</td>\n","      <td>@VirginAmerica it's really aggressive to blast...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>negative</td>\n","      <td>@VirginAmerica and it's a really big bad thing...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      label                                               text\n","0   neutral                @VirginAmerica What @dhepburn said.\n","1  positive  @VirginAmerica plus you've added commercials t...\n","2   neutral  @VirginAmerica I didn't today... Must mean I n...\n","3  negative  @VirginAmerica it's really aggressive to blast...\n","4  negative  @VirginAmerica and it's a really big bad thing..."]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"_tDY5WYlmt5y","colab_type":"code","colab":{}},"cell_type":"code","source":["df['text']=df['text'].apply(clean_tweets)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"phYlKYnMmx6x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"e17abf7b-0ae5-4231-9259-55243c6e63af","executionInfo":{"status":"ok","timestamp":1552255001032,"user_tz":0,"elapsed":695,"user":{"displayName":"Nishit Biswas","photoUrl":"","userId":"09281587797812942318"}}},"cell_type":"code","source":["#data after the clean \n","df.head()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>neutral</td>\n","      <td>What  said.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>positive</td>\n","      <td>plus you've added commercials to the experienc...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>neutral</td>\n","      <td>I didn't today... Must mean I need to take ano...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>negative</td>\n","      <td>it's really aggressive to blast obnoxious \"ent...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>negative</td>\n","      <td>and it's a really big bad thing about it</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      label                                               text\n","0   neutral                                        What  said.\n","1  positive  plus you've added commercials to the experienc...\n","2   neutral  I didn't today... Must mean I need to take ano...\n","3  negative  it's really aggressive to blast obnoxious \"ent...\n","4  negative           and it's a really big bad thing about it"]},"metadata":{"tags":[]},"execution_count":9}]},{"metadata":{"id":"GjGVXTX3oyrj","colab_type":"text"},"cell_type":"markdown","source":["It is evident that the text is now more structured and appears neater. We can now proceed to the model construction. \n","\n","---\n","\n","\n","\n","\n"]},{"metadata":{"id":"Q5MBTejZo0lY","colab_type":"text"},"cell_type":"markdown","source":["***Model Construction***\n","\n","As a first step, we use the scikit-learn library to partition the data into a training set and a test data. In this model we use a train-test split of 70/30, such that the entire data set is randomly separated into a training set (10000 samples, 70%) and a test set (4636 samples, 30%). \n","\n","Implementing ULMFiT involves:\n","\n","1.   Creating a language model with pretrained weights that you fine-tune to a new dataset\n","2.   Creating a classifier on top of the encoder of the language model\n","\n","In this context, we fine-tune a pretrained language model (trained on the Wikitext 103 dataset) to the training data set, and build a classfier for the test data set. To get the data ready for modelling, we use the *TextLMDataBunch* and *TestClasDataBunch* classes from the fastai library to prepare the data for the language model and the classification model respectively. "]},{"metadata":{"id":"KSOcNWWqsDwD","colab_type":"code","colab":{}},"cell_type":"code","source":["#path to the Wiki103 dataset used by ULMFit\n","path = untar_data(URLs.WT103_1)\n","from sklearn.model_selection import train_test_split\n","df_trn, df_val = train_test_split(df, stratify = df['label'], test_size = 0.3, random_state = 1)\n","#Language model data\n","data_lm = TextLMDataBunch.from_df(path=path, train_df = df_trn, valid_df = df_val)\n","#classifier model data\n","data_clas = TextClasDataBunch.from_df(path=path, train_df = df_trn, valid_df = df_val, vocab=data_lm.train_ds.vocab, bs=42)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oNG_GAP81hty","colab_type":"text"},"cell_type":"markdown","source":["We now create a learner object, ‘learn’, that will directly create a model, download the pre-trained weights, and be ready for fine-tuning. We can use the *data_lm* object we previously created to fine-tune. We use the AWD_LSTM architecture provided by [fast.ai](https://www.fast.ai):"]},{"metadata":{"id":"vK5AB20Liuc5","colab_type":"code","colab":{},"outputId":"28ef5e47-a9b1-410e-8133-0d743335ea20"},"cell_type":"code","source":["learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)\n","learn.fit_one_cycle(1, 1e-2)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["Total time: 05:55 <p><table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>5.424283</td>\n","      <td>4.465004</td>\n","      <td>0.216194</td>\n","      <td>05:55</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"XiHA6f6y3zum","colab_type":"text"},"cell_type":"markdown","source":["We can unfreeze the model and fine-tune it."]},{"metadata":{"id":"yAl497Vziuc8","colab_type":"code","colab":{},"outputId":"bb5a885d-d821-4894-90da-7192227355a8"},"cell_type":"code","source":["learn.unfreeze()\n","learn.fit_one_cycle(1, 1e-3)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["Total time: 12:21 <p><table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>4.118608</td>\n","      <td>3.912419</td>\n","      <td>0.275915</td>\n","      <td>12:21</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"LQF3KECZ5-u3","colab_type":"text"},"cell_type":"markdown","source":["We save the encoder to use it for classification on the test data."]},{"metadata":{"id":"NJD53j7oiuc-","colab_type":"code","colab":{}},"cell_type":"code","source":["learn.save_encoder('ft_enc')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aZq-5OcMx8Td","colab_type":"text"},"cell_type":"markdown","source":["***Classification***\n","\n","We now use the *data_clas* object we created previously to build a classifier with our fine-tuned encoder."]},{"metadata":{"id":"UMdnn_Zniuc_","colab_type":"code","colab":{}},"cell_type":"code","source":["learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\n","learn.load_encoder('ft_enc')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ilOTGKy0iudB","colab_type":"code","colab":{},"outputId":"33cd12cb-39a8-471c-aa45-8545a2d8d0ff"},"cell_type":"code","source":["learn.fit_one_cycle(1, 1e-2)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["Total time: 04:35 <p><table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.710382</td>\n","      <td>0.572643</td>\n","      <td>0.765301</td>\n","      <td>04:35</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"6jxuksGs4S_Z","colab_type":"text"},"cell_type":"markdown","source":["Again, we can unfreeze the model and fine-tune it."]},{"metadata":{"id":"bQxY5gxHiudD","colab_type":"code","colab":{},"outputId":"24d6e915-00d2-4588-8e94-35267281348c"},"cell_type":"code","source":["learn.freeze_to(-2)\n","learn.fit_one_cycle(1, slice(5e-3/2., 5e-3))"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["Total time: 06:01 <p><table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.654022</td>\n","      <td>0.537686</td>\n","      <td>0.776776</td>\n","      <td>06:01</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"OblU-A5ziudF","colab_type":"code","colab":{},"outputId":"cffd196e-9c95-4d3f-be2c-495eec2bdf02"},"cell_type":"code","source":["learn.unfreeze()\n","learn.fit_one_cycle(3, slice(2e-3/100, 2e-3))"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["Total time: 52:47 <p><table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>accuracy</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.553355</td>\n","      <td>0.513982</td>\n","      <td>0.795082</td>\n","      <td>17:52</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.572120</td>\n","      <td>0.505976</td>\n","      <td>0.798361</td>\n","      <td>17:15</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.538764</td>\n","      <td>0.475355</td>\n","      <td>0.811475</td>\n","      <td>17:40</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"YFtzYHVqpPoF","colab_type":"text"},"cell_type":"markdown","source":["---\n","**Conclusion**\n","\n","In this project, we have shown that we can achieve a high accuracy of 81% on a 70/30 train-test split. This performace is comparable to the Naive Bayes model proposed by [Duan et al](http://cs229.stanford.edu/proj2016spr/report/042.pdf)  and is substanially better than the RNN model proposed by [Yuan/Zhou](https://cs224d.stanford.edu/reports/YuanYe.pdf). For further work, we can aim to deduce the reasons behind each sentiment and better visualise the view on each US airline. We can also try using a semi-superivised implementation of ULMFiT."]}]}